# WTS : The price change in Gangnam district 


```python
import pandas as pd
import numpy as np
```

read the excel file. We use the __real estate data__ from KB LiiV .


```python
df_Gangnam = pd.read_excel('data(95to07).xlsx', sheet_name='price', header=0, skipfooter=0, usecols="A,D")
```

We want to make the new variables such as 'p_rate' , 'p_dot'  
**'p_rate'**(; the growth-rate of Gangnam area) will be used to make x   
**'p_dot'** (; the rate of change of condominium price = 100 means that the price is unchanged)


```python
df_Gangnam['L1_Gangnam'] = df_Gangnam['Gangnam'].shift(1)
```


```python
df_Gangnam['p_rate'] = (df_Gangnam['Gangnam'] / df_Gangnam['L1_Gangnam'] - 1) * 100
```


```python
df_Gangnam['p_dot'] = (df_Gangnam['Gangnam'] / df_Gangnam['L1_Gangnam']) * 100
```

we don't use 'L1_Gangnam' column anymore. __To fill the empty data of 'p-dot' & 'p-rate'__ remove 'L1_Gangnam'


```python
df_Gangnam = df_Gangnam.drop('L1_Gangnam', axis=1)
```


```python
df_Gangnam = df_Gangnam.fillna(method='bfill')
```

using the 'for' , we can make the variable 'x'


```python
df_Gangnam['x0'] = np.where(df_Gangnam['p_rate'] < 0, -1, 1)
```


```python
df_Gangnam['cum_x0'] = df_Gangnam['x0']
```


```python
for i in range(len(df_Gangnam)):
    if i == 0:
        if df_Gangnam['x0'][i] == 1:
            df_Gangnam['cum_x0'][i] = 1
        else:
            df_Gangnam['cum_x0'][i] = 0
    else:
        if df_Gangnam['x0'][i] == df_Gangnam['x0'][i - 1]:
            df_Gangnam['cum_x0'][i] = df_Gangnam['cum_x0'][i - 1] + df_Gangnam['x0'][i]
        else:
            df_Gangnam['cum_x0'][i] = df_Gangnam['x0'][i]
```

    C:\Users\a\anaconda3\lib\site-packages\ipykernel_launcher.py:4: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      after removing the cwd from sys.path.
    C:\Users\a\anaconda3\lib\site-packages\ipykernel_launcher.py:9: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      if __name__ == '__main__':
    C:\Users\a\anaconda3\lib\site-packages\ipykernel_launcher.py:11: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      # This is added back by InteractiveShellApp.init_path()
    

Make the variable 'x', 'x' means the number of periods in the current house price growth regime.


```python
df_Gangnam['x'] = df_Gangnam['cum_x0']
```


```python
df_Gangnam = df_Gangnam.drop(['x0', 'cum_x0'], axis=1)
```

**Chonsei**


```python
df_Gangnam_ch = pd.read_excel('data(95to07).xlsx', sheet_name='chonsei', header=0, skipfooter=0, usecols="D")
```


```python
df_Gangnam_ch = df_Gangnam_ch.rename({'Gangnam':'Gangnam_ch'},axis=1)
```


```python
df_Gangnam = pd.concat([df_Gangnam, df_Gangnam_ch], axis=1)
```

**Sovereign yield**


```python
df_sovereign = pd.read_excel('data(95to07).xlsx', sheet_name='sovereign yield', header=0, skipfooter=0, usecols="B")
```


```python
df_Gangnam = pd.concat([df_Gangnam, df_sovereign], axis=1)
```

Make the variable R(rent), C(rent-price ratio), b(chonesei-price ratio) , i(sovereign yield)


```python
df_Gangnam['R'] = df_Gangnam['Gangnam_ch'] * df_Gangnam['sovereign yield']
```


```python
df_Gangnam['c'] = df_Gangnam['R'] / df_Gangnam['Gangnam']
```


```python
df_Gangnam['b'] = df_Gangnam['Gangnam_ch'] / df_Gangnam['Gangnam']
```


```python
df_Gn = df_Gangnam.rename({'sovereign yield': 'i'}, axis=1)
```


```python
df_Gn['p_dot-1'] = df_Gn['p_dot'].shift(3)
```


```python
df_Gn = df_Gn.fillna(method='bfill')
```

### In the df_GB, we can identify the variables to calculate the regression.


```python
df_Gn['lnP'] = np.log(df_Gn['p_dot'])
```


```python
df_Gn['lnP-1'] = np.log(df_Gn['p_dot-1'])
```


```python
df_Gn['lnc'] = np.log(df_Gn['c'])
```


```python
df_Gn['lni'] = np.log(df_Gn['i'])
```


```python
df_Gn['lnb'] = np.log(df_Gn['b'])
```


```python
df_GN = df_Gn.drop({'Gangnam','Gangnam_ch','R','p_dot','p_dot-1','i','c','b','p_rate'}, axis=1)
```

#### The way to arrange dataframe : df = df[['e','c','b','f','d','a']]


```python
df_GN = df_GN[['lnP','lnb','lnc','lnP-1','x','lni']]
```

#### export to excel


```python
df_GN.to_excel('Gangnam_95to07.xlsx')
```

#### make the regression


```python
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib as plt
import seaborn as sns
```

Set the dependent variable (Y), independent variable(X: vector)  
## regression 1)


```python
Y = df_GN['lnP']
```


```python
X1 = df_GN[['lnc', 'lnP-1', 'x']]
```


```python
X1m = sm.add_constant(X1)
```


```python
K1m = sm.OLS(Y, X1m).fit()
```


```python
K1m.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>lnP</td>       <th>  R-squared:         </th> <td>   0.565</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.555</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   59.68</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 17 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>8.47e-25</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:47:16</td>     <th>  Log-Likelihood:    </th> <td>  430.97</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   142</td>      <th>  AIC:               </th> <td>  -853.9</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   138</td>      <th>  BIC:               </th> <td>  -842.1</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>    5.9889</td> <td>    0.285</td> <td>   21.047</td> <td> 0.000</td> <td>    5.426</td> <td>    6.551</td>
</tr>
<tr>
  <th>lnc</th>   <td>   -0.0064</td> <td>    0.002</td> <td>   -3.499</td> <td> 0.001</td> <td>   -0.010</td> <td>   -0.003</td>
</tr>
<tr>
  <th>lnP-1</th> <td>   -0.2972</td> <td>    0.062</td> <td>   -4.826</td> <td> 0.000</td> <td>   -0.419</td> <td>   -0.175</td>
</tr>
<tr>
  <th>x</th>     <td>    0.0023</td> <td>    0.000</td> <td>   12.411</td> <td> 0.000</td> <td>    0.002</td> <td>    0.003</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>10.671</td> <th>  Durbin-Watson:     </th> <td>   1.045</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.005</td> <th>  Jarque-Bera (JB):  </th> <td>  25.013</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.129</td> <th>  Prob(JB):          </th> <td>3.70e-06</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 5.040</td> <th>  Cond. No.          </th> <td>2.23e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.23e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.



## regression 2)


```python
X2 = df_GN[['lnc','lnP-1','x','lni']]
```


```python
X2m = sm.add_constant(X2)
```


```python
K2m = sm.OLS(Y,X2m).fit()
```


```python
K2m.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>lnP</td>       <th>  R-squared:         </th> <td>   0.599</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.587</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   51.06</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 17 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>2.97e-26</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:47:16</td>     <th>  Log-Likelihood:    </th> <td>  436.72</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   142</td>      <th>  AIC:               </th> <td>  -863.4</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   137</td>      <th>  BIC:               </th> <td>  -848.7</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>    6.1786</td> <td>    0.280</td> <td>   22.076</td> <td> 0.000</td> <td>    5.625</td> <td>    6.732</td>
</tr>
<tr>
  <th>lnc</th>   <td>    0.0164</td> <td>    0.007</td> <td>    2.358</td> <td> 0.020</td> <td>    0.003</td> <td>    0.030</td>
</tr>
<tr>
  <th>lnP-1</th> <td>   -0.3353</td> <td>    0.060</td> <td>   -5.550</td> <td> 0.000</td> <td>   -0.455</td> <td>   -0.216</td>
</tr>
<tr>
  <th>x</th>     <td>    0.0022</td> <td>    0.000</td> <td>   12.024</td> <td> 0.000</td> <td>    0.002</td> <td>    0.003</td>
</tr>
<tr>
  <th>lni</th>   <td>   -0.0301</td> <td>    0.009</td> <td>   -3.398</td> <td> 0.001</td> <td>   -0.048</td> <td>   -0.013</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>11.642</td> <th>  Durbin-Watson:     </th> <td>   1.153</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.003</td> <th>  Jarque-Bera (JB):  </th> <td>  20.337</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.362</td> <th>  Prob(JB):          </th> <td>3.84e-05</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.707</td> <th>  Cond. No.          </th> <td>2.31e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.31e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.



## regression 3)


```python
X3 = df_GN[['lnb', 'lnP-1', 'x', 'lni']]
```


```python
X3m = sm.add_constant(X3)
```


```python
K3m = sm.OLS(Y,X3m).fit()
```


```python
K3m.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>lnP</td>       <th>  R-squared:         </th> <td>   0.599</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.587</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   51.06</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 17 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>2.97e-26</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:47:16</td>     <th>  Log-Likelihood:    </th> <td>  436.72</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   142</td>      <th>  AIC:               </th> <td>  -863.4</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   137</td>      <th>  BIC:               </th> <td>  -848.7</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>    6.1786</td> <td>    0.280</td> <td>   22.076</td> <td> 0.000</td> <td>    5.625</td> <td>    6.732</td>
</tr>
<tr>
  <th>lnb</th>   <td>    0.0164</td> <td>    0.007</td> <td>    2.358</td> <td> 0.020</td> <td>    0.003</td> <td>    0.030</td>
</tr>
<tr>
  <th>lnP-1</th> <td>   -0.3353</td> <td>    0.060</td> <td>   -5.550</td> <td> 0.000</td> <td>   -0.455</td> <td>   -0.216</td>
</tr>
<tr>
  <th>x</th>     <td>    0.0022</td> <td>    0.000</td> <td>   12.024</td> <td> 0.000</td> <td>    0.002</td> <td>    0.003</td>
</tr>
<tr>
  <th>lni</th>   <td>   -0.0137</td> <td>    0.003</td> <td>   -4.934</td> <td> 0.000</td> <td>   -0.019</td> <td>   -0.008</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>11.642</td> <th>  Durbin-Watson:     </th> <td>   1.153</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.003</td> <th>  Jarque-Bera (JB):  </th> <td>  20.337</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.362</td> <th>  Prob(JB):          </th> <td>3.84e-05</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.707</td> <th>  Cond. No.          </th> <td>2.27e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.27e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.


